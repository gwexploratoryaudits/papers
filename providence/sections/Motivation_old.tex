\subsection{Motivation}
\label{sec:prov_motivation}
The \Minerva stopping condition tests the tail ratios of the probability distributions\footnote{given the alternate and null hypotheses respectively} of winner votes $K_j$ in round $j$. Note that these distributions are not conditioned on $k_{j-1}$. Thus each may be viewed as a weighted average of the corresponding conditional distributions---each conditioned on a possible value of $K_{j-1}$, with the weights corresponding to the probabilities of drawing that value of $K_{j-1}$. (See \cite{usenix_minerva} for more details). 

The proof of \Minerva's risk-limiting property relies on the fact that, at every stopping point, the tail of the probability distribution function given $H_0$ is at most $\alpha$ times that given $H_a$. However, because these distributions are averaged over the various values of $K_{j-1}$, this approach to the proof works only if $n_j$ is the same for all values of $K_{j-1}$. If not, the average distribution will not correctly represent the distribution of $K_j$. 

For example, if the current sample is used to determine a next round size of a certain fixed stopping probability, small values of $K_{j-1}$ would be followed by larger values of $n_j$.  In this case, the pdf for the small values of $K_{j-1}$ should not contribute to the average distribution function of $K_j$ for smaller values of $n_j$. This is the reason why the round schedule is predetermined for \Minerva, in order to ensure in an enforceable manner that all values of $k_{j-1}$ lead to the same value of $n_j$ if the audit does not stop in round $j-1$. 

For the \Providence audit we test the tail ratios of the distributions conditioned on $k_{j-1}$ and multiplied by the weighting factor. That is, we test this tail ratio separately for each weighted distribution corresponding to each possible value of $K_{j-1}$. Because of this, we no longer require that all values of $k_{j-1}$ should be followed by the same values of $n_j$ if the audit does not stop in round $j-1$. This results in a great deal of flexibility in multiple round audits. 

\begin{definition}
Let $d \in \{0, a\}$ stand for the null or the alternative hypothesis respectively.
Let $s_{i, d, [n_1, \ldots, n_r]}$ be the probability of observing $i$ ballots for the winner after drawing the $r^{th}$ round of an audit with round schedule $[n_1, \ldots, n_r]$ conditioned on hypothesis $H_d$ \textit{i.e.,} 
\[
s_{i, d, [n_1, \ldots, n_r]} = P\left[K_r = i | (n_1, \ldots, n_r), H_d\right]. 
%  = s_{i, d, [n_r]},
\]
For $r = 1$ 
\[
 s_{i, d, [n_r]} = \mathsf{Bin}(n_r, p_d, i).
\]
\end{definition}


\begin{example}[Adaptive round size selection] 
Let us assume a malicious auditor with the following strategy for \Minerva audit:
\begin{itemize}
 \item select second round size $n_2$ if $k_{1} = k^* = k^{p_a, p_0, \alpha}_{min, 1} - 1$,
 \item select round size $n'_2$ for $k_1 \leq k^{p_a, p_0, \alpha}_{min, 1} - 2$.
\end{itemize}
This leads to the situation where the true value of $s_{i, d, [n_1, n_2]}$ is missing any contributions from $K_1=k^*$.

\[
 s^{k^*, up}_{i, d, [n_1, n_{2}]} = \sum_{j = x}^{\min\{i, k^{p_a, p_0, \alpha}_{min, 1}-1\}} Bin(n_{2} - n_1, p_d, i - j) \cdot s_{j, d, [n_1]},
\]

\[
 s^{k^*, down}_{i, d, [n_1, n'_{2}]} = \sum_{j = \max\{0, i - (n'_2 - n_1)\}}^{k^*-1} Bin(n'_{2} - n_1, p_d, i - j) \cdot s_{j, d, [n_1]},
\]

One would need to to prove that for $n_2, n'_2$ and for any choice of $k^*$ the following holds:
\[
\sum_{i = k^{p_a, p_0, \alpha}_{min, 2, [n_1, n_2]}}^{n_r} s^{k^*, up}_{i, 0, [n_1, n_2]} \le \alpha \sum_{i = k^{p_a, p_0, \alpha}_{min, 2, [n_1, n_2]}}^{n_r} s^{k^*}_{i, A, [n_1, n_2]}.
\]
\end{example}


